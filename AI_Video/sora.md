## 提问

### 事物

> sora是什么？干什么用的？应用场景？整活场景？

sora是OpenAI推出的文字生成视频的系统，宣称可以<u>最多生成60秒的视频</u>（[来源](https://x.com/OpenAI/status/1758192957386342435)）

致敬传奇吃面王威尔史密斯https://www.youtube.com/watch?v=XQr4Xklqzw8，连一刻也没有为传奇吃面王哀悼，立刻赶到战场的是Sora

https://www.youtube.com/watch?v=TRIrr3Aar44 进度条6分04

应用场景：

- 自然语言生成视频

输入简单的文本描述，比如“在沙滩上散步的男人”， AI 会理解文本中的内容、场景和意图，生成高度一致的短视频。（OpenAI油管上有很多示例，素材可以选那上面的）

- 结合图像生成技术

Sora 使用了类似 DALL-E 和其他生成模型的技术，可以生成高质量的场景、人物和物体，并将这些静态图像合成到流畅的视频中

### 技术

> sora背后有哪些技术？又是如何训练出来的？有哪些特性？

（参考 https://blog.mlq.ai/openai-sora-overview/）

OpenAI的 **Sora** 文本生成视频模型基于先进的机器学习架构，如 **Transformer** 和 **扩散模型（Diffusion Model）**，能够从文本提示生成连贯的视频。

模型的生成过程采用 **扩散式生成**：从一开始带有噪声的图像逐步去噪，使图像逐渐逼近文本描述。Sora 类似于 OpenAI 的 DALL-E 图像生成模型，将视频帧分解成“视觉块”（visual patches），类似语言模型的“词汇单元”，这样既保证了视频的高质量输出，同时能处理各种长度、分辨率和比例的视频。

- 时序一致性

生成视频最大的挑战之一是要保持连续帧之间的时序一致性，使动作流畅。Sora 通过改进扩散模型中的时间建模，使帧与帧之间能够根据时间顺序合理变化，确保物体和背景元素在帧序列中的位置与姿态逐渐变化，以实现连贯的视频效果。

- 文本到视觉映射

模型使用了一种叫做文本到视觉映射（text-to-visual mapping）的技术，将文字描述转化为视觉特征。这种技术基于 CLIP（Contrastive Language-Image Pretraining）等模型，通过大量的跨模态数据训练，使模型能够理解“沙滩”、“日落”等语义词汇并将其映射到合适的视觉表现上。（这里的举例也可以根据最终选择的素材进行切换）

- 视频生成与融合

在生成每一帧图像后，Sora 会将这些静态图像合成为视频。在视频生成阶段，模型会利用生成对抗网络（GAN）和卷积神经网络（CNN），结合时间信息逐帧生成，从而形成自然的动作和场景转换效果。

### 思想

sora涉及的人工智能算法有哪些？

### 基础科学

这些算法需要什么数学基础？